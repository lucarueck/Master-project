- Baseline Model: Google's Temporal Fusion Transformer, LSTM or something similar which can be found in a modular implementation and can be extended easily

Build different time series models based on this baseline:
1. One simple model without enhancements
2. One model with prior synthetic data generation for underrepresented groups by using SMOTE (based on CA-GAN work)
3. Multiple models that enhances the baseline model with regularization terms in the optimization function (based on regularization works of my seminar paper)
4. Multiple models that use oversampling/undersampling strategies (based on BAHT work)
5. One model with a sampling strategy (based on FairTP)
6. One model that excludes all sensitive information from model training (based on STEER)
7. Check if long term fairness work can be adopted         --> Achieving Long Term fairness in sequential decision making
8. Check if there is sth like information transfer from difficult to easy to learn samples  --> FairSTG


Our possible own approach:
1. Check if data contains underrepresented groups, if yes use best representation bias mitigation technique from above (sampling, synthetic data generation) 
2. Use the best fairness regularization term in the optimization function


Works that are not used in my current benchmark plan:
- HiMoE (Mixture of Experts), too complicated to reconstruct the model 
- FairFor (currently to complicated, 4 steps, adversarial learning + representation learning)