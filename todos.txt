meeting notes:
- check impact of prediction length (on fairness), check suggested prediction lengths in literature
- check impact of alpha in my metric (experiments)

1. rechecking/adjusting/fixing
2. use new datasets
3. execute experiments (effects, hyperparameters)



Answers to last meeting questions:
Information from tutorial (https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-indepth.html): 

- There are known and past covariates: known covariates are known for the entire forecasting horizon, past covariates only up to the start of the forecast horizon
specified columns in TimeSeriesPredctor are known covariates, the remaining columns will automatically be interpreted as past covariates

- weekend dtype: the original dtype is bool, the looking into the code once can see that int, bool and float are interpreted as continuous variabales, while object, string and
category are interpreted as categorical variables --> couldn't find out why bool is interpreted as continous (maybe due to programming efficency reasons, as it can be converted to 0/1),
    I suggest to convert it to a categorical variable manually for the experiments, as for me its more categorical

- note on metric results of autogluon in evaluate function:
        .. note::
            Metrics are always reported in 'higher is better' format.
            This means that metrics such as MASE or MAPE will be multiplied by -1, so their values will be negative.
            This is necessary to avoid the user needing to know the metric to understand if higher is better when
            looking at the evaluation results.
    --> my suggestion multiply results with -1 again, I compared the results with MAE of sklearn and they return the same values
    --> in their implementation their is a class parameter which defaults multiplys the results with -1, my implementation is correct and is multiplied by -1 internally

- Four different training presets: fast_training, medium_quality, high_quality, best_quality (differences in amout of models trained and training time)

- Own models can be added to autogluon Time Series Predictor in the 'hyperparameters' parameter, implementation of a custom model is 
complex and will take some time (minimum 2-4 hours), source: https://auto.gluon.ai/dev/tutorials/timeseries/advanced/forecasting-custom-model.html

- Model zoo (list of 25 models): https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-model-zoo.html
    --> According to ChatGPT there are only two methods using bootstraping (NPTSModel, ZeroModel)


Current Questions:
- What did you meant with bootstraping?



General Points:
- Use ideas/parts of existing work (regularization, sampling, synthetic data generation)
- Define unified Dataset Format and Task Format
- Decide on Evaluation metrics
- time series methods should be universal applicable: for every domain, univariate/multivariate forecasting, handling 
multiple bias generating attributes, processing multiple input features, (best would be if the biased features do not have to be known in advance) 
- Write Prof. Heckmann for access to Lab


long term goal: 
- Compare around 5 approaches for around 10 datasets
- develop an own method (maybe not included in project)


Other Information:
- library pygan (CGAN) --> WGAN-GP*
- MIMIC-IV demo dataset is also available, maybe also look at Freiburg dataset from BA thesis
- How to standardize multiple time series --> standardize each time series alone and keep mean/var for each to transform it back later



-------------------------------------------------------------------------------------
Current Benchmark Plan
-------------------------------------------------------------------------------------

- Baseline Model: Autogluon TimeSeriesPredictor, Google's Temporal Fusion Transformer, LSTM or something similar 
                    which can be found in a modular implementation and can be extended easily

Build different time series models based on this baseline:
1. One simple model without enhancements
2. One model with prior synthetic data generation for underrepresented groups by using SMOTE (based on CA-GAN work)
3. Multiple models that enhances the baseline model with regularization terms in the optimization function (based on regularization works of my seminar paper)
4. Multiple models that use oversampling/undersampling strategies (based on BAHT work)
5. One model with a sampling strategy that choooses data performing worse more frequently (based on FairTP)
6. One model that excludes all sensitive information from model training (based on STEER)
7. Check if long term fairness work can be adopted         --> Achieving Long Term fairness in sequential decision making
8. Check if there is sth like information transfer from difficult to easy to learn samples  --> FairSTG


Our possible own approach:
1. Check if data contains underrepresented groups, if yes use best representation bias mitigation technique from above (sampling, synthetic data generation) 
2. Use the best fairness regularization term in the optimization function


Works that are not used in my current benchmark plan:
- HiMoE (Mixture of Experts), too complicated to reconstruct the model 
- FairFor (currently to complicated, 4 steps, adversarial learning + representation learning)



---------------------------------------------------------------------------------------
Usable Parts
---------------------------------------------------------------------------------------
Existing works:
- Achieving Long Term fairness in sequential decision making
    - Metrics for long- and short-term fairness (see evaluation.py compute statistics)
        - Short term fairness: Misst, wie stark sich der Modell-Output ändert, wenn man die sensitive Variable künstlich von 0 auf 1 setzt, während alle anderen Features gleich bleiben.
        - Long-Term fairness: Misst Fairness in einer längeren kausalen Kette, also nach mehreren Zwischenschritten, bei denen vorherige Outputs wieder zu Inputs werden können.
    - maybe the sequential decision making framework can be adapted to time series forecasting, but I currently don't understand in depth how it works
    --> some parts can be useful
    
- CA GAN
    - no model named CA GAN can be found, only baselines smote and WGAN-GP, it is not clear which model is CA GAN
    - the data named "data_ca_gan" used in the evaluations are not created or provided in any of the files
    - models and data can currently not be reconstructed from the provided code (if it can, code can be used)
    --> some parts can be useful

- FairST (EquiTensor)
    - already outperformed
    - I can see fairness objective in code but do not understand them fully
    --> it's unlikely that parts can be used

- FairFor
    - Github repo seems to be incomplete, can not find FairFor model implementation
    --> nothing can be used

- Fairness in LDS
    - Learning objectives can be useful
    - LDS are applicable to time series of different lengths (not a frequently appearing use case, only model that can to this)
    --> some parts can be useful

- Steer (Learning Time Series EHR)
    - very complex code, not reproducible as data is missing
    --> it's unlikely that parts can be used



Existing works without an implementation:
- BAHT
    - use simple methods for bias mitigation, the approach can be reconstructed
    
- FairTP
    - try to reconstruct targeted sampling, but could be complicated

- SA-Net
    - fairness objective MAPE can be adopted

- Absolute Correlation regularization
    - two fairness objective for regularization can be adopted

- FairSTG
    - too complicated and not suitable as the approach focus on spatial data

- HiMoE
    - too complicated to reconstruct the model
